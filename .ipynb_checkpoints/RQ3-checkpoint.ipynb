{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:2533: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(obj)\n",
      "/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:2247: RuntimeWarning: Mean of empty slice.\n",
      "  mns = a.mean(axis=axis)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/anaconda3/lib/python3.6/site-packages/matplotlib/tight_layout.py:177: UserWarning: The left and right margins cannot be made large enough to accommodate all axes decorations. \n",
      "  warnings.warn('The left and right margins cannot be made large '\n",
      "/anaconda3/lib/python3.6/site-packages/matplotlib/tight_layout.py:177: UserWarning: The left and right margins cannot be made large enough to accommodate all axes decorations. \n",
      "  warnings.warn('The left and right margins cannot be made large '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x1800 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Therefore, generally we got right skewed distribution when taxi is in same borough. Data is right skewed\\nbecause generally people make short trips in NY. However if you switch your borough, there should be pile up.\\nAveragely It takes half an hour. '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct 24 16:55:48 2018\n",
    "\n",
    "@author: Melis\n",
    "\"\"\"\n",
    "\n",
    "# RQ3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "dur_loc = pd.DataFrame()\n",
    "duration_min = pd.Series()\n",
    "PULocationID = pd.Series()\n",
    "DOLocationID = pd.Series()\n",
    "\n",
    "buffer = 10000\n",
    "\n",
    "for month in ['01','02','03','04','05','06']: #['01']  ['01','02','03','04','05','06']\n",
    "    for chunk in pd.read_csv(\"/Users/ince/Desktop/uni/adm/hws/hw2/yellow_tripdata_2018-\"+month+\".csv\", chunksize=buffer, nrows=100000):\n",
    "                \n",
    "        date_tmp_pickup = pd.to_datetime(chunk['tpep_pickup_datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        date_tmp_dropoff = pd.to_datetime(chunk['tpep_dropoff_datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        #x = pd.to_datetime(\"2/1/2018 0:44\", format=\"%d/%m/%Y %H:%M\")-pd.to_datetime(\"1/1/2018 23:34\", format=\"%d/%m/%Y %H:%M\")\n",
    "        \n",
    "        duration_min = duration_min.append(round((date_tmp_dropoff-date_tmp_pickup).dt.total_seconds()/60,1))\n",
    "        PULocationID = PULocationID.append(chunk['PULocationID'])\n",
    "        DOLocationID = DOLocationID.append(chunk['DOLocationID'])\n",
    "\n",
    "\n",
    "dur_loc['duration_min'] = duration_min\n",
    "dur_loc['PULocationID'] = PULocationID\n",
    "dur_loc['DOLocationID'] = DOLocationID\n",
    "dur_loc.dropna(inplace=True)      \n",
    "\n",
    "\n",
    "zone_lookup = pd.read_csv(\"/Users/ince/Desktop/uni/adm/hws/hw2/taxi_zone_lookup.csv\") #this dataset has a map btwn LocationID and Borough(ilçe)\n",
    "\n",
    "#merge dur_loc and zone_lookup to get pick up borough\n",
    "trips = dur_loc.merge(zone_lookup[['LocationID', 'Borough']], how='inner',\n",
    "                      left_on='PULocationID',\n",
    "                      right_on='LocationID').fillna(\"\")[['duration_min','PULocationID','DOLocationID','Borough']]\n",
    "\n",
    "trips.rename(columns= {'Borough':'PUBorough'}, inplace=True)\n",
    "\n",
    "#merge trips and zone_lookup to get drop off borough\n",
    "trips = trips.merge(zone_lookup[['LocationID', 'Borough']], how='inner',\n",
    "                      left_on='DOLocationID',\n",
    "                      right_on='LocationID').fillna(\"\")[['duration_min','PULocationID','DOLocationID','PUBorough','Borough']]\n",
    "\n",
    "trips.rename(columns= {'Borough':'DOBorough'}, inplace=True)\n",
    "\n",
    "\n",
    "#take a look trips and try to see if there is weird data\n",
    "trips.describe()\n",
    "\n",
    "trips.drop(trips.index[np.where(trips['duration_min'] <= 0)], inplace=True) #drop if duration of trips is zero or less than zero\n",
    "\n",
    "#trips[trips['duration_min']>1400] #there are some outliers also.\n",
    "\n",
    "\n",
    "# set of all boroughs\n",
    "Boroughs = list(set(zone_lookup['Borough']))\n",
    "Boroughs[Boroughs.index('Staten Island')] = 'StatenIsland'\n",
    "del Boroughs[Boroughs.index('Unknown')] \n",
    "\n",
    "# split data based on boroughs\n",
    "Bronx = trips[(trips['PUBorough']==trips['DOBorough']) & (trips['PUBorough']=='Bronx')]['duration_min']\n",
    "Brooklyn = trips[(trips['PUBorough']==trips['DOBorough']) & (trips['PUBorough']=='Brooklyn')]['duration_min']\n",
    "EWR = trips[(trips['PUBorough']==trips['DOBorough']) & (trips['PUBorough']=='EWR')]['duration_min']\n",
    "Manhattan = trips[(trips['PUBorough']==trips['DOBorough']) & (trips['PUBorough']=='Manhattan')]['duration_min']\n",
    "Queens = trips[(trips['PUBorough']==trips['DOBorough']) & (trips['PUBorough']=='Queens')]['duration_min']\n",
    "StatenIsland = trips[(trips['PUBorough']==trips['DOBorough']) & (trips['PUBorough']=='StatenIsland')]['duration_min']\n",
    "SwitchedBorough = trips[trips['PUBorough']!=trips['DOBorough']]['duration_min']\n",
    "#SwitchedBorough can be specified as finding the most frequest trip among 2 boroughs\n",
    "\n",
    "\n",
    "#find and eliminate outliers for each borough since we have huge data, visialization will be easier in that way.\n",
    "    \n",
    "#for visual check, boxplot will be helpful;\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#create a dataframe and collect together all boroughs to easily visialize them\n",
    "df = pd.DataFrame()\n",
    "for B in Boroughs:\n",
    "    df2 = eval(\"pd.DataFrame({'Duration':\" + B +\",'Borough':['\"+str(B)+\"']*len(\"+B+\")})\")\n",
    "    df = pd.concat([df, df2])\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame({'Duration':SwitchedBorough,'Borough':['SwitchedBorough']*len(SwitchedBorough)})])\n",
    "\n",
    "    \n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.boxenplot(x=\"Borough\", y=\"Duration\",\n",
    "              color=\"b\", order= Boroughs+['SwitchedBorough'],\n",
    "              scale=\"linear\", data=df)\n",
    "\n",
    "\"\"\"it can be obviously seen that there are some outliers on data, especially for Manhattan, Bronx, Brooklyn\n",
    "and Queens. Generally outliers are above 1400 minites. We may ignore them since we have too much data. Also\n",
    "We can observe the distribution of data more effectively\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"z-score is for normalization. After normalized the data, 99.6 percent of data should be lie \n",
    "between -3 and 3. In the light of this information, if the some observations are out of that range,\n",
    "they are outliers. \"\"\"\n",
    "\n",
    "#formal check by using z-score\n",
    "from scipy import stats\n",
    "threshold = 3 #the threshold is 3(we'll take absolute of it)\n",
    "\n",
    "Bronx_z = np.abs(stats.zscore(Bronx))\n",
    "Bronx.drop(Bronx.index[np.where(Bronx_z > 3)], inplace=True) #np.where(Bronx_z > 3)[0][0] BOŞ KÜME DE OLABİLİR?\n",
    "\n",
    "Brooklyn_z = np.abs(stats.zscore(Brooklyn))\n",
    "Brooklyn.drop(Brooklyn.index[np.where(Brooklyn_z > 3)], inplace=True) \n",
    "\n",
    "EWR_z = np.abs(stats.zscore(EWR))\n",
    "EWR.drop(EWR.index[np.where(EWR_z > 3)], inplace=True)\n",
    "\n",
    "Manhattan_z = np.abs(stats.zscore(Manhattan))\n",
    "Manhattan.drop(Manhattan.index[np.where(Manhattan_z > 3)], inplace=True)\n",
    "\n",
    "Queens_z = np.abs(stats.zscore(Queens))\n",
    "Queens.drop(Queens.index[np.where(Queens_z > 3)], inplace=True)\n",
    "\n",
    "StatenIsland_z = np.abs(stats.zscore(StatenIsland))\n",
    "StatenIsland.drop(StatenIsland.index[np.where(StatenIsland_z > 3)], inplace=True)\n",
    "\n",
    "SwitchedBorough_z = np.abs(stats.zscore(SwitchedBorough))\n",
    "SwitchedBorough.drop(SwitchedBorough.index[np.where(SwitchedBorough_z > 3)], inplace=True)\n",
    "\n",
    "\n",
    "#after getting rid of the outliers, create a dataframe again\n",
    "df_wo_outliers = pd.DataFrame()\n",
    "for B in Boroughs:\n",
    "    df2 = eval(\"pd.DataFrame({'Duration':\" + B +\",'Borough':['\"+str(B)+\"']*len(\"+B+\")})\")\n",
    "    df_wo_outliers = pd.concat([df_wo_outliers, df2])\n",
    "df_wo_outliers = pd.concat([df_wo_outliers,\n",
    "                            pd.DataFrame({'Duration':SwitchedBorough,'Borough':['SwitchedBorough']*len(SwitchedBorough)})])\n",
    "\n",
    "#check visually again;\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.boxenplot(x=\"Borough\", y=\"Duration\",\n",
    "              color=\"b\", order= Boroughs+['SwitchedBorough'],\n",
    "              scale=\"linear\", data=df_wo_outliers)\n",
    "\n",
    "\"\"\"it is more clear now. According to boxplot, Manhattan has the most outliers. Inlike Manhattan, Bronx\n",
    "has just one outlier. Median of the trip duration is similar for Manhattan, Bronx, Brooklyn and Queens.\n",
    "Moreover in Manhattan, people tend to make short trips more. SwitchedBorough data spreads more because\n",
    "generally it takes more time when you travel out of borough. That why it has also many outliers. \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#visiualization for all dist\n",
    "\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Initialize the FacetGrid object\n",
    "pal = sns.cubehelix_palette(8, rot=-.25, light=.7) #Make a sequential palette from the cubehelix system. bu renk seçiyo birinden öbürüne geçen bi skala ile\n",
    "Borough = sns.FacetGrid(df_wo_outliers, row=\"Borough\", hue=\"Borough\", aspect=10, palette=pal,xlim=(0, 30)) #Multi-plot grid for plotting conditional relationships.\n",
    "#burdaki row ve hue(bi de col var ama bunu kullanmadık burda):Variables that define subsets of the data, which will be drawn on separate facets in the grid. _order komutuyla sırasını belirleyebilirsin istersen\n",
    " \n",
    "\n",
    "# Draw the densities in a few steps\n",
    "\n",
    "# sns.FacetGrid() açıyosun ilk önce. boş bi zemine şablon oturttuğunu düşün. daha sonra .map() diyince çizdiriyosun detayları verip\n",
    "#kde plot: Fit and plot a univariate or bivariate kernel density estimate.\n",
    "Borough.map(sns.kdeplot, \"Duration\", clip_on=False, shade=True, alpha=1, lw=1.5, bw=.2) \n",
    "Borough.map(sns.kdeplot, \"Duration\", clip_on=False, color=\"w\", lw=2, bw=.2)\n",
    "Borough.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates--buna gerek yok\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "\n",
    "Borough.map(label, \"Duration\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "Borough.fig.subplots_adjust(hspace=-.25)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "Borough.set_titles(\"\")\n",
    "Borough.set(yticks=[])\n",
    "Borough.despine(bottom=True, left=True)\n",
    "\n",
    "\"\"\"It is good prepreliminary preparation to have an idea about how all distributions look like. \"\"\"\n",
    "\n",
    "\"\"\"We may guess the which distribution can be good fit to our data but it is just a guess. Based on our\n",
    "\"instinct\", we can check some distribution whether they are similar to our data or not. In order to\n",
    "find that, some error calculation and graphically check will be used\"\"\"\n",
    "\n",
    "import warnings\n",
    "import scipy.stats as st\n",
    "import statsmodels as sm\n",
    "import matplotlib\n",
    "\n",
    "\"\"\"After deeply understanding the steps,\n",
    "the source: https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python?lq=1\n",
    "will be used for taking help.\n",
    "\n",
    "Empirically we'll try to fit some distribution to our data for each boroughs. Then the best parameters'll\n",
    "find for the specified distribution. Sum of square error(SSE) will be used when the distributions are compared\n",
    "to eachother. Thus, the distribution with minimum SSE will be the best fit.\n",
    "\"\"\"\n",
    "\n",
    "# Build a function to find best distribution that fit the data.\n",
    "def bestDist(data, bins=200):\n",
    "    # Get histogram of original data, according to number of bins, histogram function will create boundaries for bins based on its width\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0 #since there are 100 bins, we split the bins equally on x axis.\n",
    "                   \n",
    "    #in scipy, there are lots of distribution type but in here, we just try basic ones;\n",
    "    DISTRIBUTIONS = [ st.beta,st.cauchy,st.chi2,st.expon,st.exponnorm,st.gamma,st.laplace,st.logistic,\n",
    "                     st.lognorm,st.norm,st.pareto,st.rayleigh,st.uniform ]\n",
    "            \n",
    "    # Best holders; just for the inital step, we assign some samples\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "        \n",
    "    # We try all distribution that we wrote above to estimate distribution parameters from our original data\n",
    "    for dist in DISTRIBUTIONS:\n",
    "        # Try to fit the distribution\n",
    "        try:\n",
    "            # Ignore warnings from data that can't be fit\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "        \n",
    "                # fit dist to data\n",
    "                prms = dist.fit(data) #fit helps to find best prm.s for the data\n",
    "        \n",
    "                #Separate parts of parameters: we split location,scale parameter and arguments if they exist\n",
    "                arg = prms[:-2]\n",
    "                loc = prms[-2]\n",
    "                scale = prms[-1]\n",
    "        \n",
    "                \"\"\"After finding best parameters for the specified distribution, we'll calculate fitted pdf\n",
    "                and sse(sum of square error)\"\"\"\n",
    "                pdf = dist.pdf(x, loc=loc, scale=scale, *arg) #it generates pdf from x; remember that we generate x from our original data to fit the histogram\n",
    "                sse = np.sum(np.power(y - pdf, 2)) #sum of square error. this is our decision criteria\n",
    "                \n",
    "                # if axis pass in add to plot\n",
    "                try:\n",
    "                    if ax:\n",
    "                        pd.Series(pdf, x).plot(ax=ax)\n",
    "                    end\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "                # Update best holders for the distribution and its parameters according to sse.\n",
    "                if best_sse > sse > 0:\n",
    "                    best_dist = dist\n",
    "                    best_prms = prms\n",
    "                    best_sse = sse\n",
    "        \n",
    "        except Exception:\n",
    "            pass\n",
    "    return (best_dist.name, best_prms)\n",
    "\n",
    "# Store the best distribution and its parameters, then we'll use them to get pdf\n",
    "    \n",
    "def pdf(best_dist, best_prms, size=1000):\n",
    "    # Separate parts of parameters\n",
    "    arg = best_prms[:-2]\n",
    "    loc = best_prms[-2]\n",
    "    scale = best_prms[-1]\n",
    "\n",
    "    # Get sane start and end points of distribution\n",
    "    \"\"\"ppf:Percent point function (inverse of cdf — percentiles)\n",
    "       we use start end point to split x axis based on percentiles. 0.001 means it will almost start from\n",
    "       0th index and it goes until reach almost 100th percentile (0.999 represents that)\"\"\"\n",
    "    #if arg doesn't exist we just use location and scale prm.s, otherwise we use tree of them.\n",
    "    start = best_dist.ppf(0.001, *arg, loc=loc, scale=scale) if arg else best_dist.ppf(0.001, loc=loc, scale=scale)\n",
    "    end = best_dist.ppf(0.999, *arg, loc=loc, scale=scale) if arg else best_dist.ppf(0.999, loc=loc, scale=scale)\n",
    "\n",
    "    # Now we can create pdf\n",
    "    x = np.linspace(start, end, size) #.linspace() returns evenly spaced numbers over a specified interval\n",
    "    y = best_dist.pdf(x, loc=loc, scale=scale, *arg) \n",
    "    pdf = pd.Series(y, x)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "#Let's try it for all boroughs\n",
    "    \n",
    "# Manhattan;\n",
    "Man_best_fit_name, Man_best_fit_prms = bestDist(Manhattan, bins=200)\n",
    "Man_best_dist = getattr(st, Man_best_fit_name) #getattr() returns the value of the named attribute of an object\n",
    "\n",
    "Man_pdf = pdf(Man_best_dist, Man_best_fit_prms)\n",
    "\n",
    "Man_prm = list(map(lambda x: \"%.3f\" % x, Man_best_fit_prms)) #for the title round the parameters\n",
    "\n",
    "#Let's check on histogram and line of pdf\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "Man_pdf.plot(lw=2, label='pdf', legend=True, ax=ax)\n",
    "ax2 = ax.twinx()\n",
    "sns.distplot(Manhattan, bins=50,kde=False, color=\"g\", ax=ax2)\n",
    "\n",
    "plt.xlabel(u'Duration (min)')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(u'The best fit distribution for Manhattan\\n'+Man_best_fit_name+' with parameters \\n' \n",
    "          +'('+Man_prm[0]+','+Man_prm[1]+','+Man_prm[2]+')') # You can comment this line out if you don't need title\n",
    "plt.show(fig)\n",
    "\n",
    "\"\"\"It seems lognormal distribution totally fit the Manhattan taxi travel duration. \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Bronx\n",
    "Brnx_best_fit_name, Brnx_best_fit_prms = bestDist(Bronx, bins=100)\n",
    "Brnx_best_dist = getattr(st, Brnx_best_fit_name) #getattr() returns the value of the named attribute of an object\n",
    "\n",
    "Brnx_pdf = pdf(Brnx_best_dist, Brnx_best_fit_prms)\n",
    "\n",
    "Brnx_prm = list(map(lambda x: \"%.3f\" % x, Brnx_best_fit_prms)) #for the title round the parameters\n",
    "\n",
    "#Let's check on histogram and line of pdf\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "Brnx_pdf.plot(lw=2, label='pdf', legend=True, ax=ax)\n",
    "ax2 = ax.twinx()\n",
    "sns.distplot(Bronx, bins=50,kde=False, color=\"g\", ax=ax2)\n",
    "\n",
    "plt.xlabel(u'Duration (min)')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(u'The best fit distribution for Bronx\\n'+Brnx_best_fit_name+' with parameters \\n' \n",
    "          +'('+Brnx_prm[0]+','+Brnx_prm[1]+','+Brnx_prm[2]+')') # You can comment this line out if you don't need title\n",
    "plt.show(fig)\n",
    "\n",
    "\"\"\"This time lognormal distribution almost fit the Bronx taxi travel duration. But if we compare with\n",
    "Manhattan, this one is close to fit it. \"\"\" \n",
    "\n",
    "\n",
    "\n",
    "# Brooklyn\n",
    "Bklyn_best_fit_name, Bklyn_best_fit_prms = bestDist(Brooklyn, bins=200)\n",
    "Bklyn_best_dist = getattr(st, Bklyn_best_fit_name) #getattr() returns the value of the named attribute of an object\n",
    "\n",
    "Bklyn_pdf = pdf(Bklyn_best_dist, Bklyn_best_fit_prms)\n",
    "\n",
    "Bklyn_prm = list(map(lambda x: \"%.3f\" % x, Bklyn_best_fit_prms)) #for the title round the parameters\n",
    "\n",
    "#Let's check on histogram and line of pdf\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "Bklyn_pdf.plot(lw=2, label='pdf', legend=True, ax=ax)\n",
    "ax2 = ax.twinx()\n",
    "sns.distplot(Brooklyn, bins=50,kde=False, color=\"g\", ax=ax2)\n",
    "\n",
    "plt.xlabel(u'Duration (min)')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(u'The best fit distribution for Brooklyn\\n'+Bklyn_best_fit_name+' with parameters \\n' \n",
    "          +'('+Bklyn_prm[0]+','+Bklyn_prm[1]+','+Bklyn_prm[2]+')') # You can comment this line out if you don't need title\n",
    "plt.show(fig)\n",
    "\n",
    "\"\"\"Again lognormal distribution is the most close one to Brooklyn taxi travel duration data.\n",
    "It seems perfect for our data. In addition to Manhattan, Brooklyn is also good example for \n",
    "lognormal distribution. \"\"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Queens\n",
    "Qns_best_fit_name, Qns_best_fit_prms = bestDist(Queens, bins=200)\n",
    "Qns_best_dist = getattr(st, Qns_best_fit_name) #getattr() returns the value of the named attribute of an object\n",
    "\n",
    "Qns_pdf = pdf(Qns_best_dist, Qns_best_fit_prms)\n",
    "\n",
    "Qns_prm = list(map(lambda x: \"%.3f\" % x, Bklyn_best_fit_prms)) #for the title round the parameters\n",
    "\n",
    "#Let's check on histogram and line of pdf\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "Qns_pdf.plot(lw=2, label='pdf', legend=True, ax=ax)\n",
    "ax2 = ax.twinx()\n",
    "sns.distplot(Queens, bins=50,kde=False, color=\"g\", ax=ax2)\n",
    "\n",
    "plt.xlabel(u'Duration (min)')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(u'The best fit distribution for Queens\\n'+Qns_best_fit_name+' with parameters \\n' \n",
    "          +'('+Qns_prm[0]+','+Qns_prm[1]+','+Qns_prm[2]+')') # You can comment this line out if you don't need title\n",
    "plt.show(fig)\n",
    "\n",
    "\"\"\"Since the Queens data suddenly decreases until 25 minutes of travel duration, we got exponentially \n",
    "normal distribution, it is also called exponentially modified Gaussian (EMG) distribution. We may say that\n",
    "Queens is about to fit it. \"\"\" \n",
    "\n",
    "\n",
    "\"\"\"EWR and Staten Island data are not too much. So no need to estimate their distribution. For example,\n",
    "EWR is 28 observations and we don't have any data for Staten Island. \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"The last one is SwitchedBorough. That means we'll examine if the taxi goes from the borough to\n",
    "another borough. We expect that duration of trips will be long all the time. Hence, most probably\n",
    "we won't get some right tail distribution like lognormal. \"\"\"\n",
    "\n",
    "# SwitchedBorough\n",
    "SBoro_best_fit_name, SBoro_best_fit_prms = bestDist(SwitchedBorough, bins=200)\n",
    "SBoro_best_dist = getattr(st, SBoro_best_fit_name) #getattr() returns the value of the named attribute of an object\n",
    "\n",
    "SBoro_pdf = pdf(SBoro_best_dist, SBoro_best_fit_prms)\n",
    "\n",
    "SBoro_prm = list(map(lambda x: \"%.3f\" % x, Bklyn_best_fit_prms)) #for the title round the parameters\n",
    "\n",
    "#Let's check on histogram and line of pdf\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "SBoro_pdf.plot(lw=2, label='pdf', legend=True, ax=ax)\n",
    "ax2 = ax.twinx()\n",
    "sns.distplot(SwitchedBorough, bins=50,kde=False, color=\"g\", ax=ax2)\n",
    "\n",
    "plt.xlabel(u'Duration (min)')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(u'The best fit distribution when trip is between two boroughs\\n'+SBoro_best_fit_name+' with parameters \\n' \n",
    "          +'('+SBoro_prm[0]+','+SBoro_prm[1]+','+SBoro_prm[2]+')') # You can comment this line out if you don't need title\n",
    "plt.show(fig)\n",
    "\n",
    "\"\"\"Since the trips are cumulated at some point, around 25 minutes, we got exponentially normal distribution.\n",
    "That make sense. Because it is easy to guess if you are switch the borough, it will take more time. So there\n",
    "are no much data for short trip. Furthermore, exponentially normal is also good fit for the data. \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Therefore, generally we got right skewed distribution when taxi is in same borough. Data is right skewed\n",
    "because generally people make short trips in NY. However if you switch your borough, there should be pile up.\n",
    "Averagely It takes half an hour. \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
